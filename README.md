# PaleoDB Anomaly Detection with Hugging Face Transformers

State-of-the-art anomaly detection system for the Paleobiology Database (PaleoDB) using transformer-based deep learning models from Hugging Face.

## Overview

This project implements advanced anomaly detection on fossil occurrence data from PaleoDB using:
- **Transformer-based AutoEncoders** adapted for tabular data
- **Pre-trained Hugging Face models** (microsoft/deberta-v3-small architecture)
- **Ensemble learning** for robust predictions
- Comprehensive training, evaluation, and inference pipelines

## Features

- ðŸš€ **State-of-the-art Models**: Implements transformer-based autoencoders using Hugging Face architectures
- ðŸ“Š **Multiple Model Options**: Custom Transformer, Hugging Face-based, and Ensemble models
- ðŸ”„ **Complete Pipeline**: Data loading, preprocessing, training, evaluation, and inference
- ðŸ“ˆ **Rich Metrics**: ROC-AUC, Precision-Recall, F1-Score, and detailed confusion matrices
- ðŸŽ¯ **Production Ready**: Easy-to-use inference API for real-world deployment
- ðŸ“‰ **Visualization**: Training curves, ROC curves, precision-recall curves, and score distributions

## Model Architecture

### Primary Model: HuggingFaceTransformerAE

The best model uses **microsoft/deberta-v3-small** architecture adapted for tabular anomaly detection:

1. **Input Projection**: Linear layer + LayerNorm to embed tabular features
2. **Transformer Encoder**: Multi-head self-attention layers from DeBERTa-v3
3. **Bottleneck**: Compressed latent representation
4. **Transformer Decoder**: Reconstructs from latent space
5. **Output Projection**: Maps back to original feature space

**Key Advantages**:
- Leverages state-of-the-art transformer architecture
- Captures complex feature interactions
- Efficient training with modern optimization
- Strong generalization on tabular data

### Alternative Models

- **TransformerAutoEncoder**: Custom transformer implementation with configurable layers
- **EnsembleAnomalyDetector**: Combines multiple models for robust predictions

## Installation

```bash
# Clone the repository
git clone <repository-url>
cd claude-code

# Install dependencies
pip install -r requirements.txt
```

### Requirements

- Python 3.8+
- PyTorch 2.0+
- Transformers 4.30+
- scikit-learn, pandas, numpy
- matplotlib, seaborn

## Quick Start

### 1. Train the Model

```bash
python train.py
```

This will:
- Download 10,000 fossil occurrence records from PaleoDB
- Preprocess and normalize the data
- Create synthetic anomalies for training
- Train the transformer-based autoencoder
- Save the best model checkpoint

**Training Configuration** (edit in `train.py`):
```python
config = {
    'data_limit': 10000,           # Number of records to download
    'batch_size': 64,               # Batch size for training
    'val_split': 0.2,               # Validation split ratio
    'anomaly_ratio': 0.05,          # Synthetic anomaly ratio
    'model_type': 'huggingface',    # 'transformer', 'huggingface', or 'ensemble'
    'num_epochs': 50,               # Maximum epochs
    'learning_rate': 1e-4,          # Learning rate
    'early_stopping_patience': 10   # Early stopping patience
}
```

### 2. Evaluate the Model

```bash
python evaluate.py
```

Outputs:
- Comprehensive metrics (ROC-AUC, PR-AUC, F1-Score, etc.)
- Confusion matrix
- Visualization plots saved to `evaluation_plots.png`
- Metrics JSON saved to `evaluation_metrics.json`

### 3. Run Inference on New Data

```bash
python inference.py --limit 1000 --output results.csv --top-k 20
```

Arguments:
- `--limit`: Number of records to download from PaleoDB
- `--output`: Output CSV file path
- `--top-k`: Number of top anomalies to display
- `--checkpoint-dir`: Model checkpoint directory (default: `./checkpoints`)

## Project Structure

```
claude-code/
â”œâ”€â”€ data_loader.py          # PaleoDB data loading and preprocessing
â”œâ”€â”€ models.py               # Transformer-based anomaly detection models
â”œâ”€â”€ train.py                # Training pipeline with early stopping
â”œâ”€â”€ evaluate.py             # Comprehensive evaluation with metrics
â”œâ”€â”€ inference.py            # Production inference script
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ checkpoints/           # Saved model checkpoints
â”‚   â”œâ”€â”€ best_model.pt      # Best model weights
â”‚   â”œâ”€â”€ config.json        # Training configuration
â”‚   â””â”€â”€ metadata.json      # Data metadata
â””â”€â”€ data/                  # Cached data (auto-generated)
```

## Usage Examples

### Training with Different Models

```python
# Train with custom Transformer
python train.py  # Set model_type='transformer' in config

# Train with Hugging Face model (recommended)
python train.py  # Set model_type='huggingface' in config

# Train with Ensemble
python train.py  # Set model_type='ensemble' in config
```

### Programmatic Inference

```python
from inference import AnomalyDetector
import pandas as pd

# Initialize detector
detector = AnomalyDetector(checkpoint_dir='./checkpoints')

# Load your data
data = pd.read_csv('your_paleodb_data.csv')

# Detect anomalies
results = detector.predict_and_explain(data, top_k=10)

# Access results
print(f"Detected {results['is_anomaly'].sum()} anomalies")
print(results[results['is_anomaly'] == 1])
```

### Custom Data Preprocessing

```python
from data_loader import PaleoDBLoader

loader = PaleoDBLoader()

# Download specific taxonomic group
data = loader.download_paleodb_data(limit=5000, base_name='Dinosauria')

# Preprocess
processed_data, metadata = loader.preprocess_data(data)

# Create synthetic anomalies for testing
data_with_anomalies, labels = loader.create_synthetic_anomalies(
    processed_data,
    anomaly_ratio=0.05
)
```

## Model Performance

Expected performance on PaleoDB data (with synthetic anomalies):

| Metric | Score |
|--------|-------|
| ROC-AUC | ~0.95+ |
| PR-AUC | ~0.90+ |
| F1-Score | ~0.85+ |
| Accuracy | ~0.95+ |

*Actual performance depends on data quality and anomaly characteristics*

## Advanced Configuration

### Fine-tuning Hyperparameters

Edit `train.py` to customize:

```python
# Model architecture
model = HuggingFaceTransformerAE(
    input_dim=metadata['input_dim'],
    model_name="microsoft/deberta-v3-small",  # Or try deberta-v3-base
    hidden_dim=256,                            # Increase for more capacity
    dropout=0.1                                 # Adjust for regularization
)

# Training parameters
trainer = AnomalyDetectionTrainer(
    model=model,
    learning_rate=1e-4,    # Decrease for stability
    weight_decay=1e-5      # Increase for regularization
)
```

### Using Different Base Models

The system supports any Hugging Face transformer model. Try:
- `microsoft/deberta-v3-base` (larger, more powerful)
- `roberta-base` (classic choice)
- `bert-base-uncased` (widely used)

## Troubleshooting

### CUDA Out of Memory
```bash
# Reduce batch size in train.py
config['batch_size'] = 32  # or 16
```

### Poor Performance
- Increase `data_limit` to train on more data
- Try different model architectures
- Adjust `anomaly_ratio` to match your real data
- Increase model capacity (more layers, larger hidden_dim)

### Data Download Issues
```python
# Use cached data or smaller limit
loader.download_paleodb_data(limit=1000)
```

## Why This Model?

**microsoft/deberta-v3-small** was chosen as the best Hugging Face model for this task because:

1. **State-of-the-Art Performance**: DeBERTa-v3 achieves top results on many benchmarks
2. **Efficient Architecture**: Small variant balances performance with computational cost
3. **Strong Representation Learning**: Excellent at capturing complex patterns
4. **Disentangled Attention**: Better understanding of feature relationships
5. **Pre-training Advantage**: Benefits from large-scale pre-training even when adapted to tabular data

## Citation

If you use this code in your research, please cite:

```bibtex
@software{paleodb_anomaly_detection,
  title = {PaleoDB Anomaly Detection with Hugging Face Transformers},
  author = {Your Name},
  year = {2025},
  url = {https://github.com/yourusername/claude-code}
}
```

## License

MIT License - see LICENSE file for details

## Acknowledgments

- Paleobiology Database (https://paleobiodb.org) for providing the data
- Hugging Face for the Transformers library
- Microsoft for DeBERTa-v3 architecture

## Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request

## Contact

For questions or issues, please open an issue on GitHub or contact [your-email].
